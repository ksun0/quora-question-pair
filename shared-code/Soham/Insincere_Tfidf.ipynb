{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q = train_data['question_text'].values\n",
    "test_q = test_data['question_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [02:36<00:00, 8358.53it/s]\n",
      "100%|██████████| 56370/56370 [00:06<00:00, 8342.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_q_tokenized = [word_tokenize(ques.lower()) for ques in tqdm(train_q)]\n",
    "test_q_tokenized = [word_tokenize(ques.lower()) for ques in tqdm(test_q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:05<00:00, 229598.37it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 472348.14it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_train = [[q for q in ques if q not in stopwords] for ques in tqdm(train_q_tokenized)]\n",
    "filtered_test = [[q for q in ques if q not in stopwords] for ques in tqdm(test_q_tokenized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ques = [\" \".join(ques) for ques in filtered_train]\n",
    "test_ques = [\" \".join(ques) for ques in filtered_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uni = TfidfVectorizer(max_features=300000)\n",
    "vectorizer_bi = TfidfVectorizer(ngram_range=(1,2),max_features=300000)\n",
    "vectorizer_tri = TfidfVectorizer(ngram_range=(1,3),max_features=300000)\n",
    "vectorizer_4gram = TfidfVectorizer(ngram_range=(1,4),max_features=300000)\n",
    "vectorizer_5gram = TfidfVectorizer(ngram_range=(1,5),max_features=300000)\n",
    "vectorizer_6gram = TfidfVectorizer(ngram_range=(1,6),max_features=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_unigram = vectorizer_uni.fit_transform(train_ques)\n",
    "BoW_bigram = vectorizer_bi.fit_transform(train_ques)\n",
    "BoW_trigram = vectorizer_tri.fit_transform(train_ques)\n",
    "BoW_4gram = vectorizer_4gram.fit_transform(train_ques)\n",
    "BoW_5gram = vectorizer_5gram.fit_transform(train_ques)\n",
    "BoW_6gram = vectorizer_6gram.fit_transform(train_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=25,n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_uni_bow = svd.fit_transform(BoW_unigram)\n",
    "truncated_bi_bow = svd.fit_transform(BoW_bigram)\n",
    "truncated_tri_bow = svd.fit_transform(BoW_trigram)\n",
    "truncated_4_bow = svd.fit_transform(BoW_4gram)\n",
    "truncated_5_bow = svd.fit_transform(BoW_5gram)\n",
    "truncated_6_bow = svd.fit_transform(BoW_6gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 194974)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(BoW_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(truncated_tri_bow, train_data['target'].values))\n",
    "np.random.shuffle(zipped)\n",
    "X = [data[0] for data in zipped]\n",
    "Y = [data[1] for data in zipped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_class1 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)\n",
    "nn_class2 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)\n",
    "nn_class3 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)\n",
    "nn_class4 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)\n",
    "nn_class5 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)\n",
    "nn_class6 = MLPClassifier(hidden_layer_sizes=(100,100,100,),activation='relu',solver='adam',learning_rate_init=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_classifier1 = nn_class1.fit(X[:1000000],Y[:1000000])\n",
    "# mlp_classifier2 = nn_class2.fit(X[:1000000],Y[:1000000])\n",
    "mlp_classifier3 = nn_class3.fit(X[:1000000],Y[:1000000])\n",
    "# mlp_classifier4 = nn_class4.fit(X[:1000000],Y[:1000000])\n",
    "# mlp_classifier5 = nn_class5.fit(X[:1000000],Y[:1000000])\n",
    "# mlp_classifier6 = nn_class6.fit(X[:1000000],Y[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data['target'].values)*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(list(zip(X[:1000000],Y[:1000000])), open('10k_train.pkl','wb'))\n",
    "pkl.dump(list(zip(X[1000000:],Y[1000000:])), open('10k_test.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(mlp_classifier, open('model_10k_mlp_100100100.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_classifier.predict(X[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.count_nonzero(y_pred - Y[:1000000])/len(Y[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincere_q = train_data.loc[train_data['target'] == 0]\n",
    "insincere_q = train_data.loc[train_data['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sincere questions: \" + str(len(sincere_q)))\n",
    "print(\"insincere questions: \" + str(len(insincere_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sincere_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insincere = \" \".join(insincere_q['question_text'].values)\n",
    "all_sincere = \" \".join(sincere_q['question_text'].values)\n",
    "in_tokenized = cust_tokenize(all_insincere.lower())\n",
    "in_tokenized = [x for x in in_tokenized if x not in [',','.','?',';','!']]\n",
    "sin_tokenized = cust_tokenize(all_sincere.lower())\n",
    "sin_tokenized = [x for x in sin_tokenized if x not in [',','.','?',';','!']]\n",
    "sincere_uni_counts = FreqDist(ngrams(sin_tokenized, 1))\n",
    "insincere_uni_counts = FreqDist(ngrams(in_tokenized, 1))\n",
    "sincere_bi_counts = FreqDist(ngrams(sin_tokenized, 2))\n",
    "insincere_bi_counts = FreqDist(ngrams(in_tokenized, 2))\n",
    "sincere_tri_counts = FreqDist(ngrams(sin_tokenized, 3))\n",
    "insincere_tri_counts = FreqDist(ngrams(in_tokenized, 3))\n",
    "sincere_4gram_counts = FreqDist(ngrams(sin_tokenized, 4))\n",
    "insincere_4gram_counts = FreqDist(ngrams(in_tokenized, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_keys = insincere_uni_counts.keys()\n",
    "sin_keys = sincere_uni_counts.keys()\n",
    "for key in list(in_keys):\n",
    "    if(len(key[0]) < 2):\n",
    "        del insincere_uni_counts[key]\n",
    "    if(key[0] in stopwords):\n",
    "        del insincere_uni_counts[key]\n",
    "        \n",
    "for key in list(sin_keys):\n",
    "    if(len(key[0]) < 2):\n",
    "        del sincere_uni_counts[key]\n",
    "    if(key[0] in stopwords):\n",
    "        del sincere_uni_counts[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincere_uni_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincere_bi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincere_tri_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sincere_4gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insincere_uni_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insincere_bi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insincere_tri_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insincere_4gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
